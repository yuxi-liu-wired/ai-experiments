{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b460ece-06d9-4436-a9cb-a4e67be56c5d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed primarily at Meta's [Fundamental AI Research](https://ai.facebook.com/) group.\n",
    "\n",
    "[facebookresearch/faiss: A library for efficient similarity search and clustering of dense vectors.](https://github.com/facebookresearch/faiss)\n",
    "\n",
    "## Installation\n",
    "\n",
    "The simplest installation is just `pip install faiss-gpu`, or `pip install faiss-cpu`. The GPU version is faster, but has more dependencies and more likely to fail.\n",
    "\n",
    "### Tests\n",
    "\n",
    "We test that the installed FAISS works. The `simple_check_faiss` generates 1000 random vectors in $\\mathbb R^{32}$, then check that each vector is closest to itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6cce170-41f2-4ad5-ae6d-29760a9960e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple check passed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def simple_check_faiss():\n",
    "    dimension = 32  # Dimension of the vector\n",
    "    num_vectors = 1000  # Number of vectors\n",
    "\n",
    "    # Generate some random vectors\n",
    "    # np.random.seed(2)  # For reproducibility\n",
    "    vectors = np.random.random((num_vectors, dimension)).astype('float32')\n",
    "\n",
    "    # Create a Faiss index\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "    # Add vectors to the index\n",
    "    index.add(vectors)\n",
    "\n",
    "    # Perform a search\n",
    "    num_neighbors = 6  # Number of nearest neighbors to return\n",
    "    _, indices = index.search(vectors, num_neighbors)  # Search for the first vector\n",
    "    \n",
    "    assert np.array_equal(indices[:, 0], np.arange(num_vectors))\n",
    "    print(f\"Simple check passed.\")\n",
    "    \n",
    "simple_check_faiss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3226b33-7583-4154-bb20-aeeb0ee15ef0",
   "metadata": {},
   "source": [
    "Next, we construct a simple semantic search index over some arXiv papers. The data is stored in a `json` file `arxiv-metadata-10000.json`. It has 10000 lines, each of which is a `json` object containing metadata for a paper on arXiv. For example, the first line is (after formatting for easy reading):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"0704.0001\",\n",
    "  \"submitter\": \"Pavel Nadolsky\",\n",
    "  \"authors\": \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
    "  \"title\": \"Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies\",\n",
    "  \"comments\": \"37 pages, 15 figures; published version\",\n",
    "  \"journal-ref\": \"Phys.Rev.D76:013009,2007\",\n",
    "  \"doi\": \"10.1103/PhysRevD.76.013009\",\n",
    "  \"report-no\": \"ANL-HEP-PR-07-12\",\n",
    "  \"categories\": \"hep-ph\",\n",
    "  \"license\": null,\n",
    "  \"abstract\": \"  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified ...\",\n",
    "  \"versions\": [ { \"version\": \"v1\", \"created\": \"Mon, 2 Apr 2007 19:18:42 GMT\" }, { \"version\": \"v2\", \"created\": \"Tue, 24 Jul 2007 20:10:27 GMT\" } ],\n",
    "  \"update_date\": \"2008-11-26\",\n",
    "  \"authors_parsed\": [ [ \"Balázs\", \"C.\", \"\" ], [ \"Berger\", \"E. L.\", \"\" ], [ \"Nadolsky\", \"P. M.\", \"\" ], [ \"Yuan\", \"C. -P.\", \"\" ] ]}\n",
    "```\n",
    "\n",
    "On my machine, DistilBERT runs at 15 entry/sec on CPU, and 100 entry/sec on GPU. It would take 5 hours to compile the entire Arxiv dataset of 1.7 million papers. The entire dataset is found in [arXiv Dataset | Kaggle](https://www.kaggle.com/datasets/Cornell-University/arxiv). It would take up 3.7 GB of space. Since this is a toy project you should not run it on the whole database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d1009e83-5ccf-4ebf-b581-01bbf777f20a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 625/625 [02:10<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10000 papers to the FAISS index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DistilBERT setup\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model = model.to(device)  # Move the model to the GPU\n",
    "\n",
    "# vectorize text batches\n",
    "def vectorize_texts(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}  # Move the inputs to the GPU\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().detach().numpy()  # Move the result back to the CPU\n",
    "\n",
    "# FAISS index setup\n",
    "dimension = 768  # DistilBERT output dimension\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Extract and vectorize data\n",
    "db_filename = 'arxiv-metadata-10000.json'\n",
    "num_lines = 10000\n",
    "\n",
    "batch_size = 16\n",
    "with open(db_filename, 'r') as f:\n",
    "    for _ in tqdm(range(num_lines // batch_size)):\n",
    "        lines = [f.readline() for _ in range(batch_size)]\n",
    "        papers = [json.loads(line) for line in lines]\n",
    "\n",
    "        # Extract the papers' titles and abstracts\n",
    "        texts = [f\"{paper['title']}: {paper['abstract']}\" for paper in papers]\n",
    "\n",
    "        # Create vectors using DistilBERT\n",
    "        vectors = vectorize_texts(texts)\n",
    "\n",
    "        # Add the vectors to the FAISS index\n",
    "        index.add(np.array(vectors).astype('float32'))\n",
    "\n",
    "print(f\"Added {num_lines} papers to the FAISS index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "918aeb3b-ac49-4fcf-a2c9-605fc3a01cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Comparison of Dissipative Particle Dynamics and Langevin thermostats for out-of-equilibrium simulations of polymeric systems\n",
      "Abstract: In this work we compare and characterize the behavior of Langevin and Dissipative Particle Dynamics (DPD) thermostats in a broad range of non-equilibrium simulations of polymeric systems. Polymer brushes in relative sliding motion, polymeric liquids in Poiseuille and Couette flows, and brush-melt interfaces are used as model systems to analyze the efficiency and limitations of different Langevin and DPD thermostat implementations. Widely used coarse-grained bead-spring models under good and poor solvent conditions are employed to assess the effects of the thermostats. We considered equilibrium, transient, and steady state examples for testing the ability of the thermostats to maintain constant temperature and to reproduce the underlying physical phenomena in non-equilibrium situations. The common practice of switching-off the Langevin thermostat in the flow direction is also critically revisited. The efficiency of different weight functions for the DPD thermostat is quantitatively analyzed as a function of the solvent quality and the non-equilibrium situation. \n",
      "\n",
      "Title: On the long range correlations of thermodynamic systems out of equilibrium\n",
      "Abstract: Experiments show that macroscopic systems in a stationary nonequilibrium state exhibit long range correlations of the local thermodynamic variables. In previous papers we proposed a Hamilton-Jacobi equation for the nonequilibrium free energy as a basic principle of nonequilibrium thermodynamics. We show here how an equation for the two point correlations can be derived from the Hamilton-Jacobi equation for arbitrary transport coefficients for dynamics with both external fields and boundary reservoirs. In contrast with fluctuating hydrodynamics, this approach can be used to derive equations for correlations of any order. Generically, the solutions of the equation for the correlation functions are non-trivial and show that long range correlations are indeed a common feature of nonequilibrium systems. Finally, we establish a criterion to determine whether the local thermodynamic variables are positively or negatively correlated in terms of properties of the transport coefficients. \n",
      "\n",
      "Title: The First Law of Thermodynamics and the Thermodynamic Description of Elastic Solids\n",
      "Abstract: Historically, the thermodynamic behavior of gasses was described first and the derived equations were adapted to solids. It is suggested that the current thermodynamic description of solid phase is still incomplete because the isothermal work done on or by the system is not counted in the internal energy. It is also suggested that the isobaric work should not be deducted from the internal energy because the system does not do work when it expands. Further more it is suggested that Joule postulate regarding the mechanical equivalency of heat -the first law of thermodynamics- is not universal and not applicable to elastic solids. The equations for the proposed thermodynamic description of solids are derived and tested by calculating the internal energies of the system using the equation of state of MgO. The agreement with theory is good. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search(query, k=5):\n",
    "    vector = vectorize_texts([query])\n",
    "    _, indices = index.search(np.array(vector).astype('float32'), k)\n",
    "    return indices[0]\n",
    "\n",
    "def load_paper_details():\n",
    "    with open(db_filename, 'r') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "paper_details = load_paper_details()\n",
    "\n",
    "import latexcodec\n",
    "def print_paper_details(line_numbers):\n",
    "    for i in line_numbers:\n",
    "        paper = paper_details[i]\n",
    "        title = paper['title'].replace('\\n', ' ').encode().decode('latex')\n",
    "        print(f\"Title: {title}\")\n",
    "        abstract = paper['abstract'].replace('\\n', ' ').encode().decode('latex')\n",
    "        print(f\"Abstract: {abstract}\\n\")\n",
    "\n",
    "# Test the function\n",
    "query = \"the statistical mechanics of freezing and boiling water\"\n",
    "line_numbers = search(query, k=3)\n",
    "print_paper_details(line_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2f203-f81b-421a-8d64-5d32b4e1a30b",
   "metadata": {},
   "source": [
    "## Instructor-XL\n",
    "\n",
    "As we see, the search is okay. Let's try something else than DistilBERT: the `Instructor-base`. \n",
    "\n",
    "In short, `Instructor-base` is a language model that turns text into vectors, like DistilBERT, but unlike DistilBERT, it can be prompted. The idea is that DistilBERT is a domain-general embedder, but sometimes you need to make something domain-specific, to preserve different kinds of information. For example, an embedder for math papers should preserve distinctions between \"ball\" and \"sphere\" and \"circle\" and \"disk\", and an embedder for poetry should preserve rhyming information.\n",
    "\n",
    "There are three models: `base`, `large`, `xl` (extra-large). We use the `base` model for experimenting as it is the fastest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d26e2fa-f721-4a22-98f2-fe6fe6c9a722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deadscholar/miniconda3/envs/faiss/lib/python3.9/site-packages/InstructorEmbedding/instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "model = INSTRUCTOR('hkunlp/instructor-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bb181d-6b7d-4016-b45b-8c1574ecbe2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\n",
    "instruction = \"Represent the Science title:\"\n",
    "embeddings = model.encode([[instruction,sentence]])\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57a11727-0643-449c-8f3b-f8ca106464eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2500/2500 [03:46<00:00, 11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10000 papers to the FAISS index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)\n",
    "\n",
    "# FAISS index setup\n",
    "dimension = 768  # Instructor output dimension\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Extract and vectorize data\n",
    "db_filename = 'arxiv-metadata-10000.json'\n",
    "num_lines = 10000\n",
    "batch_size = 4\n",
    "\n",
    "# Preparation for encoding\n",
    "instructions = [\"Represent the science titles and abstracts: \"] * batch_size\n",
    "\n",
    "with open(db_filename, 'r') as f:\n",
    "    for _ in tqdm(range(num_lines // batch_size)):\n",
    "        lines = [f.readline() for _ in range(batch_size)]\n",
    "        papers = [json.loads(line) for line in lines]\n",
    "\n",
    "        # Extract the papers' titles and abstracts\n",
    "        texts = [f\"{paper['title']}: {paper['abstract']}\" for paper in papers]\n",
    "        \n",
    "        # Prepare the inputs\n",
    "        inputs = [[instr, txt] for instr, txt in zip(instructions, texts)]\n",
    "        \n",
    "        # Create vectors using Instructor-XL\n",
    "        vectors = model.encode(\n",
    "            sentences=inputs,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False,\n",
    "            convert_to_numpy=True,\n",
    "            device=str(device)\n",
    "        )\n",
    "\n",
    "        # Add the vectors to the FAISS index\n",
    "        index.add(np.array(vectors).astype('float32'))\n",
    "\n",
    "print(f\"Added {num_lines} papers to the FAISS index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75783b-be85-44ad-8b40-dae751d93253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:faiss]",
   "language": "python",
   "name": "conda-env-faiss-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
